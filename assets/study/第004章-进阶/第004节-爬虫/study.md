# ğŸ“ Python ç½‘ç»œçˆ¬è™«è¯¦è§£

## 1. ä»€ä¹ˆæ˜¯ç½‘ç»œçˆ¬è™«

**ç½‘ç»œçˆ¬è™«ï¼ˆWeb Crawler/Spiderï¼‰** æ˜¯ä¸€ç§è‡ªåŠ¨è·å–ç½‘é¡µå†…å®¹çš„ç¨‹åºï¼Œå®ƒæ¨¡æ‹Ÿæµè§ˆå™¨è¡Œä¸ºï¼Œä»äº’è”ç½‘ä¸ŠæŠ“å–æ•°æ®ã€‚çˆ¬è™«å¹¿æ³›åº”ç”¨äºæ•°æ®é‡‡é›†ã€æœç´¢å¼•æ“ã€ä»·æ ¼ç›‘æ§ã€æ–°é—»èšåˆç­‰é¢†åŸŸã€‚

## 2. çˆ¬è™«çš„åŸºæœ¬åŸç†

### 2.1 HTTPåè®®åŸºç¡€
```python
# HTTPè¯·æ±‚çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†
# 1. è¯·æ±‚è¡Œï¼šæ–¹æ³• + URL + åè®®ç‰ˆæœ¬
# 2. è¯·æ±‚å¤´ï¼šåŒ…å«å®¢æˆ·ç«¯ä¿¡æ¯
# 3. è¯·æ±‚ä½“ï¼šPOSTè¯·æ±‚çš„æ•°æ®

# å¸¸è§çš„HTTPæ–¹æ³•
# GET: è·å–èµ„æº
# POST: æäº¤æ•°æ®
# PUT: æ›´æ–°èµ„æº
# DELETE: åˆ é™¤èµ„æº
```

### 2.2 çˆ¬è™«å·¥ä½œæµç¨‹
1. **å‘é€HTTPè¯·æ±‚**ï¼šå‘ç›®æ ‡ç½‘ç«™å‘é€è¯·æ±‚
2. **æ¥æ”¶å“åº”**ï¼šè·å–HTMLã€JSONç­‰æ ¼å¼çš„æ•°æ®
3. **è§£ææ•°æ®**ï¼šæå–æ‰€éœ€çš„ä¿¡æ¯
4. **å­˜å‚¨æ•°æ®**ï¼šä¿å­˜åˆ°æ–‡ä»¶æˆ–æ•°æ®åº“
5. **å¤„ç†åçˆ¬**ï¼šåº”å¯¹éªŒè¯ç ã€IPé™åˆ¶ç­‰

## 3. åŸºç¡€åº“ä»‹ç»

### 3.1 requestsåº“
```python
import requests

# åŸºæœ¬GETè¯·æ±‚
response = requests.get('https://httpbin.org/get')
print(response.status_code)  # 200
print(response.text)  # å“åº”å†…å®¹

# å¸¦å‚æ•°çš„GETè¯·æ±‚
params = {'key1': 'value1', 'key2': 'value2'}
response = requests.get('https://httpbin.org/get', params=params)
print(response.url)  # å®Œæ•´çš„URL

# POSTè¯·æ±‚
data = {'username': 'admin', 'password': '123456'}
response = requests.post('https://httpbin.org/post', data=data)
print(response.json())

# è®¾ç½®è¯·æ±‚å¤´
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
}
response = requests.get('https://httpbin.org/headers', headers=headers)
print(response.json())
```

### 3.2 urllibåº“ï¼ˆå†…ç½®ï¼‰
```python
import urllib.request
import urllib.parse

# åŸºæœ¬è¯·æ±‚
response = urllib.request.urlopen('https://httpbin.org/get')
print(response.read().decode('utf-8'))

# å¸¦å‚æ•°çš„è¯·æ±‚
params = {'key1': 'value1', 'key2': 'value2'}
url = 'https://httpbin.org/get?' + urllib.parse.urlencode(params)
response = urllib.request.urlopen(url)
print(response.read().decode('utf-8'))

# POSTè¯·æ±‚
data = urllib.parse.urlencode({'username': 'admin'}).encode('utf-8')
request = urllib.request.Request('https://httpbin.org/post', data=data)
response = urllib.request.urlopen(request)
print(response.read().decode('utf-8'))
```

## 4. HTMLè§£æ

### 4.1 BeautifulSoupåº“
```python
from bs4 import BeautifulSoup
import requests

# è·å–ç½‘é¡µå†…å®¹
response = requests.get('https://example.com')
html_content = response.text

# è§£æHTML
soup = BeautifulSoup(html_content, 'html.parser')

# æŸ¥æ‰¾å…ƒç´ 
title = soup.find('title')
print(title.text)

# æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
links = soup.find_all('a')
for link in links:
    print(link.get('href'))

# ä½¿ç”¨CSSé€‰æ‹©å™¨
divs = soup.select('div.content')
for div in divs:
    print(div.text)

# æŸ¥æ‰¾ç‰¹å®šå±æ€§çš„å…ƒç´ 
images = soup.find_all('img', src=True)
for img in images:
    print(img['src'])
```

### 4.2 lxmlåº“
```python
from lxml import html
import requests

# è·å–ç½‘é¡µå†…å®¹
response = requests.get('https://example.com')
tree = html.fromstring(response.content)

# ä½¿ç”¨XPathæŸ¥æ‰¾å…ƒç´ 
titles = tree.xpath('//title/text()')
print(titles)

# æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
links = tree.xpath('//a/@href')
print(links)

# æŸ¥æ‰¾ç‰¹å®šclassçš„å…ƒç´ 
divs = tree.xpath('//div[@class="content"]')
for div in divs:
    print(div.text_content())
```

## 5. æ•°æ®æå–æŠ€å·§

### 5.1 æ­£åˆ™è¡¨è¾¾å¼
```python
import re

# æå–é‚®ç®±åœ°å€
text = "è”ç³»é‚®ç®±ï¼šadmin@example.com æˆ– support@test.com"
emails = re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', text)
print(emails)  # ['admin@example.com', 'support@test.com']

# æå–ç”µè¯å·ç 
text = "ç”µè¯ï¼š138-1234-5678 æˆ– 010-12345678"
phones = re.findall(r'\d{3,4}-\d{7,8}|\d{11}', text)
print(phones)  # ['138-1234-5678', '010-12345678']

# æå–HTMLæ ‡ç­¾å†…å®¹
html_text = '<div class="price">Â¥99.99</div>'
price = re.search(r'<div class="price">(.*?)</div>', html_text)
if price:
    print(price.group(1))  # Â¥99.99
```

### 5.2 JSONæ•°æ®æå–
```python
import json
import requests

# è·å–JSONæ•°æ®
response = requests.get('https://api.github.com/users/octocat')
data = response.json()

# æå–ç‰¹å®šå­—æ®µ
print(f"ç”¨æˆ·å: {data['login']}")
print(f"å§“å: {data['name']}")
print(f"å…³æ³¨è€…: {data['followers']}")

# å¤„ç†åµŒå¥—JSON
if 'company' in data:
    print(f"å…¬å¸: {data['company']}")
```

## 6. åçˆ¬è™«åº”å¯¹

### 6.1 è®¾ç½®è¯·æ±‚å¤´
```python
import requests
import random

# éšæœºUser-Agent
user_agents = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
]

headers = {
    'User-Agent': random.choice(user_agents),
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1'
}

response = requests.get('https://example.com', headers=headers)
```

### 6.2 ä½¿ç”¨ä»£ç†
```python
import requests

# ä½¿ç”¨ä»£ç†
proxies = {
    'http': 'http://proxy.example.com:8080',
    'https': 'https://proxy.example.com:8080'
}

response = requests.get('https://httpbin.org/ip', proxies=proxies)
print(response.json())

# éšæœºä»£ç†
proxy_list = [
    'http://proxy1.example.com:8080',
    'http://proxy2.example.com:8080',
    'http://proxy3.example.com:8080'
]

import random
proxy = random.choice(proxy_list)
proxies = {'http': proxy, 'https': proxy}
```

### 6.3 å¤„ç†Cookieå’ŒSession
```python
import requests

# ä½¿ç”¨Sessionä¿æŒCookie
session = requests.Session()

# ç™»å½•
login_data = {'username': 'admin', 'password': '123456'}
session.post('https://example.com/login', data=login_data)

# è®¿é—®éœ€è¦ç™»å½•çš„é¡µé¢
response = session.get('https://example.com/dashboard')
print(response.text)

# æ‰‹åŠ¨è®¾ç½®Cookie
cookies = {'session_id': 'abc123', 'user_id': '456'}
response = requests.get('https://example.com', cookies=cookies)
```

### 6.4 å»¶æ—¶å’Œé‡è¯•
```python
import time
import random
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# éšæœºå»¶æ—¶
def random_delay():
    time.sleep(random.uniform(1, 3))

# é‡è¯•æœºåˆ¶
session = requests.Session()
retry_strategy = Retry(
    total=3,
    backoff_factor=1,
    status_forcelist=[429, 500, 502, 503, 504],
)
adapter = HTTPAdapter(max_retries=retry_strategy)
session.mount("http://", adapter)
session.mount("https://", adapter)

# ä½¿ç”¨
response = session.get('https://example.com')
random_delay()
```

## 7. å®é™…åº”ç”¨ç¤ºä¾‹

### 7.1 æ–°é—»ç½‘ç«™çˆ¬è™«
```python
import requests
from bs4 import BeautifulSoup
import json
import time

def crawl_news(url):
    """çˆ¬å–æ–°é—»åˆ—è¡¨"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_list = []
        # å‡è®¾æ–°é—»åœ¨classä¸ºnews-itemçš„divä¸­
        news_items = soup.find_all('div', class_='news-item')
        
        for item in news_items:
            title_elem = item.find('h3')
            link_elem = item.find('a')
            time_elem = item.find('span', class_='time')
            
            if title_elem and link_elem:
                news = {
                    'title': title_elem.text.strip(),
                    'link': link_elem.get('href'),
                    'time': time_elem.text.strip() if time_elem else ''
                }
                news_list.append(news)
        
        return news_list
    
    except Exception as e:
        print(f"çˆ¬å–å¤±è´¥: {e}")
        return []

# ä½¿ç”¨ç¤ºä¾‹
news_url = "https://news.example.com"
news_data = crawl_news(news_url)

# ä¿å­˜åˆ°JSONæ–‡ä»¶
with open('news.json', 'w', encoding='utf-8') as f:
    json.dump(news_data, f, ensure_ascii=False, indent=2)
```

### 7.2 ç”µå•†ä»·æ ¼ç›‘æ§
```python
import requests
from bs4 import BeautifulSoup
import re
import time
from datetime import datetime

def get_product_price(url):
    """è·å–å•†å“ä»·æ ¼"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æŸ¥æ‰¾ä»·æ ¼å…ƒç´ ï¼ˆæ ¹æ®å®é™…ç½‘ç«™è°ƒæ•´é€‰æ‹©å™¨ï¼‰
        price_elem = soup.find('span', class_='price')
        if price_elem:
            price_text = price_elem.text
            # æå–æ•°å­—
            price = re.search(r'[\d,]+\.?\d*', price_text)
            if price:
                return float(price.group().replace(',', ''))
        
        return None
    
    except Exception as e:
        print(f"è·å–ä»·æ ¼å¤±è´¥: {e}")
        return None

def monitor_price(product_url, target_price):
    """ä»·æ ¼ç›‘æ§"""
    while True:
        current_price = get_product_price(product_url)
        if current_price:
            print(f"{datetime.now()}: å½“å‰ä»·æ ¼ {current_price}")
            
            if current_price <= target_price:
                print(f"ä»·æ ¼è¾¾åˆ°ç›®æ ‡ï¼å½“å‰ä»·æ ¼: {current_price}")
                break
        
        time.sleep(3600)  # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡

# ä½¿ç”¨ç¤ºä¾‹
product_url = "https://shop.example.com/product/123"
target_price = 99.99
monitor_price(product_url, target_price)
```

### 7.3 å›¾ç‰‡ä¸‹è½½å™¨
```python
import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urljoin, urlparse

def download_images(url, save_dir='images'):
    """ä¸‹è½½ç½‘é¡µä¸­çš„æ‰€æœ‰å›¾ç‰‡"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)
    
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡
        img_tags = soup.find_all('img')
        
        for i, img in enumerate(img_tags):
            img_url = img.get('src')
            if img_url:
                # å¤„ç†ç›¸å¯¹URL
                img_url = urljoin(url, img_url)
                
                # è·å–æ–‡ä»¶å
                parsed_url = urlparse(img_url)
                filename = os.path.basename(parsed_url.path)
                if not filename:
                    filename = f"image_{i}.jpg"
                
                # ä¸‹è½½å›¾ç‰‡
                try:
                    img_response = requests.get(img_url, headers=headers)
                    img_response.raise_for_status()
                    
                    filepath = os.path.join(save_dir, filename)
                    with open(filepath, 'wb') as f:
                        f.write(img_response.content)
                    
                    print(f"ä¸‹è½½æˆåŠŸ: {filename}")
                
                except Exception as e:
                    print(f"ä¸‹è½½å¤±è´¥ {img_url}: {e}")
    
    except Exception as e:
        print(f"è·å–é¡µé¢å¤±è´¥: {e}")

# ä½¿ç”¨ç¤ºä¾‹
download_images("https://example.com/gallery")
```

## 8. æ•°æ®å­˜å‚¨

### 8.1 ä¿å­˜åˆ°æ–‡ä»¶
```python
import json
import csv
import pandas as pd

# ä¿å­˜ä¸ºJSON
data = [
    {'name': 'å¼ ä¸‰', 'age': 25, 'city': 'åŒ—äº¬'},
    {'name': 'æå››', 'age': 30, 'city': 'ä¸Šæµ·'}
]

with open('data.json', 'w', encoding='utf-8') as f:
    json.dump(data, f, ensure_ascii=False, indent=2)

# ä¿å­˜ä¸ºCSV
with open('data.csv', 'w', newline='', encoding='utf-8') as f:
    writer = csv.DictWriter(f, fieldnames=['name', 'age', 'city'])
    writer.writeheader()
    writer.writerows(data)

# ä½¿ç”¨pandasä¿å­˜
df = pd.DataFrame(data)
df.to_csv('data_pandas.csv', index=False, encoding='utf-8')
df.to_excel('data.xlsx', index=False)
```

### 8.2 ä¿å­˜åˆ°æ•°æ®åº“
```python
import sqlite3
import pymongo

# SQLiteæ•°æ®åº“
def save_to_sqlite(data):
    conn = sqlite3.connect('crawled_data.db')
    cursor = conn.cursor()
    
    # åˆ›å»ºè¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS news (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT,
            content TEXT,
            url TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # æ’å…¥æ•°æ®
    for item in data:
        cursor.execute('''
            INSERT INTO news (title, content, url)
            VALUES (?, ?, ?)
        ''', (item['title'], item['content'], item['url']))
    
    conn.commit()
    conn.close()

# MongoDBæ•°æ®åº“
def save_to_mongodb(data):
    client = pymongo.MongoClient('mongodb://localhost:27017/')
    db = client['crawled_data']
    collection = db['news']
    
    collection.insert_many(data)
    client.close()
```

## 9. çˆ¬è™«æ¡†æ¶

### 9.1 Scrapyæ¡†æ¶
```python
# scrapy_example.py
import scrapy

class NewsSpider(scrapy.Spider):
    name = 'news'
    start_urls = ['https://news.example.com']
    
    def parse(self, response):
        # è§£ææ–°é—»åˆ—è¡¨
        for news in response.css('div.news-item'):
            yield {
                'title': news.css('h3::text').get(),
                'link': news.css('a::attr(href)').get(),
                'time': news.css('span.time::text').get()
            }
        
        # è·Ÿè¿›ä¸‹ä¸€é¡µ
        next_page = response.css('a.next::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)

# è¿è¡Œå‘½ä»¤: scrapy runspider scrapy_example.py -o news.json
```

### 9.2 Seleniumè‡ªåŠ¨åŒ–
```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time

def crawl_with_selenium(url):
    """ä½¿ç”¨Seleniumçˆ¬å–åŠ¨æ€å†…å®¹"""
    # è®¾ç½®Chromeé€‰é¡¹
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    
    driver = webdriver.Chrome(options=options)
    
    try:
        driver.get(url)
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "content"))
        )
        
        # æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹
        for i in range(3):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
        
        # æå–æ•°æ®
        elements = driver.find_elements(By.CLASS_NAME, "item")
        data = []
        for element in elements:
            title = element.find_element(By.TAG_NAME, "h3").text
            data.append({'title': title})
        
        return data
    
    finally:
        driver.quit()

# ä½¿ç”¨ç¤ºä¾‹
data = crawl_with_selenium("https://dynamic-site.example.com")
```

## 10. æ³•å¾‹å’Œé“å¾·è€ƒè™‘

### 10.1 robots.txtåè®®
```python
import requests
from urllib.robotparser import RobotFileParser

def check_robots_txt(url):
    """æ£€æŸ¥robots.txt"""
    rp = RobotFileParser()
    rp.set_url(url + '/robots.txt')
    rp.read()
    
    return rp.can_fetch('*', url)

# ä½¿ç”¨ç¤ºä¾‹
if check_robots_txt('https://example.com'):
    print("å…è®¸çˆ¬å–")
else:
    print("ä¸å…è®¸çˆ¬å–")
```

### 10.2 æœ€ä½³å®è·µ
1. **éµå®ˆrobots.txt**ï¼šå°Šé‡ç½‘ç«™çš„çˆ¬è™«åè®®
2. **æ§åˆ¶é¢‘ç‡**ï¼šé¿å…å¯¹æœåŠ¡å™¨é€ æˆè¿‡å¤§å‹åŠ›
3. **å°Šé‡ç‰ˆæƒ**ï¼šä¸è¦çˆ¬å–å—ç‰ˆæƒä¿æŠ¤çš„å†…å®¹
4. **æ•°æ®ä½¿ç”¨**ï¼šåˆç†ä½¿ç”¨çˆ¬å–çš„æ•°æ®
5. **éšç§ä¿æŠ¤**ï¼šä¸è¦çˆ¬å–ä¸ªäººéšç§ä¿¡æ¯

## é‡è¦æç¤º

1. **éµå®ˆæ³•å¾‹æ³•è§„**ï¼šç¡®ä¿çˆ¬è™«è¡Œä¸ºåˆæ³•
2. **å°Šé‡ç½‘ç«™è§„åˆ™**ï¼šéµå®ˆrobots.txtå’Œç½‘ç«™æ¡æ¬¾
3. **æ§åˆ¶è¯·æ±‚é¢‘ç‡**ï¼šé¿å…å¯¹æœåŠ¡å™¨é€ æˆå‹åŠ›
4. **å¤„ç†å¼‚å¸¸**ï¼šåšå¥½é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
5. **æ•°æ®è´¨é‡**ï¼šéªŒè¯å’Œæ¸…æ´—çˆ¬å–çš„æ•°æ®
6. **æ€§èƒ½ä¼˜åŒ–**ï¼šä½¿ç”¨å¼‚æ­¥è¯·æ±‚æé«˜æ•ˆç‡
7. **åçˆ¬åº”å¯¹**ï¼šåˆç†åº”å¯¹å„ç§åçˆ¬æªæ–½

# ä½ å¯ä»¥åœ¨åº•ä¸‹çš„ä»£ç ç¼–è¾‘å™¨ä¸­ï¼Œè¾“å…¥ä½ çš„ä»£ç ã€‚



# ç„¶åï¼Œç‚¹å‡»æŒ‰é’®ï¼Œäº¤ç”±AIè¯„è®º
